{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas for Python in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to show two data structures side by side\n",
    "# Used in Web McKinney's presentations: http://www.youtube.com/watch?v=w26x-z-BdWQ\n",
    "def side_by_side(*objs, **kwds):\n",
    "    from pandas.io.formats.printing import adjoin\n",
    "    space = kwds.get('space', 4)\n",
    "    reprs = [repr(obj).split('\\n') for obj in objs]\n",
    "    print(adjoin(space, *reprs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series\n",
    "### Creating Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series called upon list: ['Car', 'Bicycle', 'Bike', 'Bus']\n",
      "0        Car\n",
      "1    Bicycle\n",
      "2       Bike\n",
      "3        Bus\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "a=['Car','Bicycle','Bike','Bus'] # list of strings\n",
    "print(\"Series called upon list: {}\\n{}\".format(a,pd.Series(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series called upon array: [1 2 3 4]\n",
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "b=[1,2,3,4] # list of numbers\n",
    "c=np.array(b) # array\n",
    "print(\"Series called upon array: {}\\n{}\".format(c,pd.Series(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series called upon dictionary: {'a': 10, 'b': 20, 'c': 30, 'd': 40}\n",
      "a    10\n",
      "b    20\n",
      "c    30\n",
      "d    40\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "d={'a':10,\n",
    "  'b':20,\n",
    "  'c':30,\n",
    "  'd':40} # dictionary\n",
    "print(\"Series called upon dictionary: {}\\n{}\".format(d,pd.Series(d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Operations on Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series_1:\n",
      "Car        1\n",
      "Bicycle    2\n",
      "Bike       3\n",
      "Bus        4\n",
      "dtype: int64\n",
      "\n",
      "Series_2:\n",
      "Bike      1\n",
      "Scooty    2\n",
      "Auto      3\n",
      "Bus       4\n",
      "dtype: int64\n",
      "\n",
      "Union of Series_1 and Series_2:\n",
      "Auto       NaN\n",
      "Bicycle    NaN\n",
      "Bike       4.0\n",
      "Bus        8.0\n",
      "Car        NaN\n",
      "Scooty     NaN\n",
      "dtype: float64\n",
      "\n",
      "After dropping Nan Values:\n",
      "Bike    4.0\n",
      "Bus     8.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Union of the index\n",
    "\n",
    "series_1=pd.Series(b,a) # pd.Series(data,row_index)\n",
    "print(\"Series_1:\\n{}\".format(series_1)) \n",
    "series_2=pd.Series(data=b,index=['Bike','Scooty','Auto','Bus'])\n",
    "print(\"\\nSeries_2:\\n{}\".format(series_2))\n",
    "\n",
    "# (values of only bus and bike are added since they are common)\n",
    "print('\\nUnion of Series_1 and Series_2:\\n{}'.format(series_1+series_2))\n",
    "\n",
    "# Dropping Nan values\n",
    "print('\\nAfter dropping Nan Values:\\n{}'.format((series_1+series_2).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series x:\n",
      "Blue      15.0\n",
      "Green     10.0\n",
      "Yellow     5.0\n",
      "Orange    30.0\n",
      "Purple    20.0\n",
      "Red       25.0\n",
      "White      NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "x=pd.Series([15,10,5,30,20,25,np.nan],['Blue','Green','Yellow','Orange','Purple','Red','White'])\n",
    "print(\"Series x:\\n{}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class of Series x:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print('Class of Series x: ',type(x)) # class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of series x are returned as array using the values attribute:\n",
      "[15. 10.  5. 30. 20. 25. nan]\n",
      "\n",
      "Indexes of series x are returned using the index attribute:\n",
      "Index(['Blue', 'Green', 'Yellow', 'Orange', 'Purple', 'Red', 'White'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Values of series x are returned as array using the values attribute:\\n{}\".format(x.values)) # attribute\n",
    "print(\"\\nIndexes of series x are returned using the index attribute:\\n{}\".format(x.index)) # attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of elements of series:  105.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum of elements of series: \",x.sum()) # function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of elements of series:  11250000.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Product of elements of series: \",x.product()) # function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of elements of series:  17.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of elements of series: \",x.mean()) # function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two elements in series:\n",
      "\n",
      "Blue     15.0\n",
      "Green    10.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"First two elements in series:\\n\\n{}\".format(x.head(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last two elements in series:\n",
      "\n",
      "Red      25.0\n",
      "White     NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Last two elements in series:\\n\\n{}\".format(x.tail(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of operations:\n",
      "\n",
      "count     6.000000\n",
      "mean     17.500000\n",
      "std       9.354143\n",
      "min       5.000000\n",
      "25%      11.250000\n",
      "50%      17.500000\n",
      "75%      23.750000\n",
      "max      30.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary of operations:\\n\\n{}\".format(x.describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in series:  6\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of elements in series: \",x.count()) # count() excludes the null values while counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum in series is 30.0 at position Orange\n",
      "Minimum in series is 5.0 at position Yellow\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum in series is {} at position {}\".format(x.max(),x.idxmax())) # idxmax() returns the index of maximum element\n",
    "print(\"Minimum in series is {} at position {}\".format(x.min(),x.idxmin())) # idxmin() returns the index of minimum element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series sorted by value:\n",
      "Yellow     5.0\n",
      "Green     10.0\n",
      "Blue      15.0\n",
      "Purple    20.0\n",
      "Red       25.0\n",
      "Orange    30.0\n",
      "White      NaN\n",
      "dtype: float64\n",
      "\n",
      "Series sorted by index:\n",
      "Blue      15.0\n",
      "Green     10.0\n",
      "Orange    30.0\n",
      "Purple    20.0\n",
      "Red       25.0\n",
      "White      NaN\n",
      "Yellow     5.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Series sorted by value:\\n{}\".format(x.sort_values())) # x.sort_values() returns the series by sorting values in ascending order\n",
    "print(\"\\nSeries sorted by index:\\n{}\".format(x.sort_index())) # x.sort_index() returns the series by sorting indexes in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted list of values from series:  [30.0, 25.0, 20.0, 15.0, 10.0, 5.0, nan]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sorted list of values from series: \",sorted(x, reverse=True)) # returns the list in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of elements:\n",
      "White      NaN\n",
      "Purple    20.0\n",
      "Blue      15.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample of elements:\\n{}\".format(x.sample(3))) # picks a random sample from series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series x:\n",
      "Blue      15.0\n",
      "Green     10.0\n",
      "Yellow     5.0\n",
      "Orange    30.0\n",
      "Purple    20.0\n",
      "Red       25.0\n",
      "White      NaN\n",
      "dtype: float64\n",
      "\n",
      "15 in x: False\n",
      "'Blue' in x: True\n",
      "'Red' in x.index: True\n",
      "20 in x.values: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Series x:\\n{}\".format(x))\n",
    "\n",
    "print('\\n15 in x:',15 in x) # returns false, although 15 is present in the series as it checks for index rather than values\n",
    "print(\"'Blue' in x:\",'Blue' in x) # returns true\n",
    "print(\"'Red' in x.index:\",'Red' in x.index) # returns true, works similar to above\n",
    "print(\"20 in x.values:\",20 in x.values) # returns true as it checks in values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting values using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series x:\n",
      "Blue      15.0\n",
      "Green     10.0\n",
      "Yellow     5.0\n",
      "Orange    30.0\n",
      "Purple    20.0\n",
      "Red       25.0\n",
      "White      NaN\n",
      "dtype: float64\n",
      "\n",
      "Extracting a single value x['Orange']: 30.0\n",
      "\n",
      "Extracting multiple values x[['Green','Yellow']]:\n",
      "Green     10.0\n",
      "Yellow     5.0\n",
      "dtype: float64\n",
      "\n",
      "Extracting Series with wrong index x[['green','Yellow']]:\n",
      "green     NaN\n",
      "Yellow    5.0\n",
      "dtype: float64\n",
      "\n",
      "Extracting Series x['Green':'Purple']:\n",
      "Green     10.0\n",
      "Yellow     5.0\n",
      "Orange    30.0\n",
      "Purple    20.0\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Using the get():\n",
      "\n",
      "Returning a value x.get('Orange'):  30.0\n",
      "\n",
      "Returning multiple values: x.get(['Orange','Red'])\n",
      "Orange    30.0\n",
      "Red       25.0\n",
      "dtype: float64\n",
      "\n",
      "Extracting value with wrong index x.get('green',default='Not a valid index'): Not a valid index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shaur\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pandas\\core\\series.py:1155: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    }
   ],
   "source": [
    "print(\"Series x:\\n{}\".format(x))\n",
    "\n",
    "print(\"\\nExtracting a single value x['Orange']:\",x['Orange']) # returns value\n",
    "print(\"\\nExtracting multiple values x[['Green','Yellow']]:\\n{}\".format(x[['Green','Yellow']])) # returns a series\n",
    "print(\"\\nExtracting Series with wrong index x[['green','Yellow']]:\\n{}\".format(x[['green','Yellow']])) # returns the series with value of 'green' as NaN, indexes are case-sensitive\n",
    "print(\"\\nExtracting Series x['Green':'Purple']:\\n{}\".format(x[\"Green\":\"Purple\"]))\n",
    "\n",
    "# get() is also used to return values\n",
    "print(\"\\n\\nUsing the get():\\n\\nReturning a value x.get('Orange'): \",x.get(\"Orange\")) # returns value\n",
    "print(\"\\nReturning multiple values: x.get(['Orange','Red'])\\n{}\".format(x.get([\"Orange\",\"Red\"]))) # returns a series\n",
    "\n",
    "# default parameter is returned when no such data is present in series\n",
    "print(\"\\nExtracting value with wrong index x.get('green',default='Not a valid index'):\",x.get(\"green\",default=\"Not a valid index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series a:\n",
      "A    1\n",
      "B    1\n",
      "C    4\n",
      "D    7\n",
      "E    9\n",
      "F    3\n",
      "G    7\n",
      "H    4\n",
      "I    1\n",
      "J    9\n",
      "dtype: int64\n",
      "\n",
      "Series b:\n",
      "Blue      C\n",
      "Orange    F\n",
      "Yellow    A\n",
      "Red       D\n",
      "Green     I\n",
      "dtype: object\n",
      "\n",
      "Mapping of b on a:\n",
      "Blue      4\n",
      "Orange    3\n",
      "Yellow    1\n",
      "Red       7\n",
      "Green     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "a=pd.Series([1,1,4,7,9,3,7,4,1,9],['A','B','C','D','E','F','G','H','I','J'])\n",
    "b=pd.Series(['C','F','A','D','I'],['Blue','Orange','Yellow','Red','Green'])\n",
    "print(\"Series a:\\n{}\".format(a))\n",
    "print(\"\\nSeries b:\\n{}\".format(b))\n",
    "\n",
    "# Values of Series b are mapped with indexes of series a\n",
    "print(\"\\nMapping of b on a:\\n{}\".format(b.map(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series:\n",
      "A    1\n",
      "B    1\n",
      "C    4\n",
      "D    7\n",
      "E    9\n",
      "F    3\n",
      "G    7\n",
      "H    4\n",
      "I    1\n",
      "J    9\n",
      "dtype: int64\n",
      "\n",
      "Check whether values in series are unique:  False\n",
      "\n",
      "Unique values in series with its no of occurences:\n",
      "1    3\n",
      "9    2\n",
      "7    2\n",
      "4    2\n",
      "3    1\n",
      "dtype: int64\n",
      "\n",
      "Unique values in series with its no of occurences in ascending order:\n",
      "3    1\n",
      "4    2\n",
      "7    2\n",
      "9    2\n",
      "1    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "a=pd.Series([1,1,4,7,9,3,7,4,1,9],['A','B','C','D','E','F','G','H','I','J'])\n",
    "print(\"Series:\\n{}\".format(a))\n",
    "print(\"\\nCheck whether values in series are unique: \",a.is_unique) # returns boolean value (true/false)\n",
    "\n",
    "# value_counts returns the number of occurences of each value\n",
    "print(\"\\nUnique values in series with its no of occurences:\\n{}\".format(a.value_counts()))\n",
    "\n",
    "# ascending parameter returns the number of occurences of each value in ascending order\n",
    "print(\"\\nUnique values in series with its no of occurences in ascending order:\\n{}\".format(a.value_counts(ascending=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series:\n",
      "A    1\n",
      "B    1\n",
      "C    4\n",
      "D    7\n",
      "E    9\n",
      "F    3\n",
      "G    7\n",
      "H    4\n",
      "I    1\n",
      "J    9\n",
      "dtype: int64\n",
      "\n",
      "Adding value 10 to each element:\n",
      "A    11\n",
      "B    11\n",
      "C    14\n",
      "D    17\n",
      "E    19\n",
      "F    13\n",
      "G    17\n",
      "H    14\n",
      "I    11\n",
      "J    19\n",
      "dtype: int64\n",
      "\n",
      "Subtracting value 10 to each element:\n",
      "A   -9\n",
      "B   -9\n",
      "C   -6\n",
      "D   -3\n",
      "E   -1\n",
      "F   -7\n",
      "G   -3\n",
      "H   -6\n",
      "I   -9\n",
      "J   -1\n",
      "dtype: int64\n",
      "\n",
      "Multiplying by value 10 with each element:\n",
      "A    10\n",
      "B    10\n",
      "C    40\n",
      "D    70\n",
      "E    90\n",
      "F    30\n",
      "G    70\n",
      "H    40\n",
      "I    10\n",
      "J    90\n",
      "dtype: int64\n",
      "\n",
      "Dividing by value 10 with each element:\n",
      "A    0.1\n",
      "B    0.1\n",
      "C    0.4\n",
      "D    0.7\n",
      "E    0.9\n",
      "F    0.3\n",
      "G    0.7\n",
      "H    0.4\n",
      "I    0.1\n",
      "J    0.9\n",
      "dtype: float64\n",
      "\n",
      "Implementing apply() on series using lambda:\n",
      "A      0.2500\n",
      "B      0.2500\n",
      "C     18.0625\n",
      "D     64.0000\n",
      "E    110.2500\n",
      "F      9.0000\n",
      "G     64.0000\n",
      "H     18.0625\n",
      "I      0.2500\n",
      "J    110.2500\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "a=pd.Series([1,1,4,7,9,3,7,4,1,9],['A','B','C','D','E','F','G','H','I','J'])\n",
    "print(\"Series:\\n{}\".format(a))\n",
    "print(\"\\nAdding value 10 to each element:\\n{}\".format(a.add(10)))\n",
    "print(\"\\nSubtracting value 10 to each element:\\n{}\".format(a.sub(10)))\n",
    "print(\"\\nMultiplying by value 10 with each element:\\n{}\".format(a.mul(10)))\n",
    "print(\"\\nDividing by value 10 with each element:\\n{}\".format(a.div(10)))\n",
    "\n",
    "# apply() is used to implement some function on each element of series\n",
    "print(\"\\nImplementing apply() on series using lambda:\\n{}\".format(a.apply(lambda x:(((5*x)-3)/4)**2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1.0          A     NaN     \n",
      "B    2.0          B     NaN     \n",
      "C    3.0          C     3.0     \n",
      "D    4.0          D     4.0     \n",
      "E    5.0          E     5.0     \n",
      "F    6.0          F     6.0     \n",
      "G    7.0          G     NaN     \n",
      "H    8.0          H     NaN     \n",
      "I    9.0          I     NaN     \n",
      "P    NaN          P    14.0     \n",
      "Q    NaN          Q    15.0     \n",
      "R    NaN          R    16.0     \n",
      "Y    NaN          Y    19.0     \n",
      "Z    NaN          Z    18.0     \n",
      "dtype: float64    dtype: float64\n"
     ]
    }
   ],
   "source": [
    "s1=pd.Series([1,2,3,4,5,6,7,8,9],['A','B','C','D','E','F','G','H','I']) # pd.Series(data,row_index)\n",
    "# print(\"Series_1:\\n{}\".format(s1)) \n",
    "s2=pd.Series([4,5,6,14,15,16,3,18,19],['D','E','F','P','Q','R','C','Z','Y'])\n",
    "# print(\"\\nSeries_2:\\n{}\".format(s2))\n",
    "\n",
    "# Align s1 and s2 with an outer join = default\n",
    "a, b = s1.align(s2, join='outer')\n",
    "side_by_side(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C    3          C    3      \n",
      "D    4          D    4      \n",
      "E    5          E    5      \n",
      "F    6          F    6      \n",
      "dtype: int64    dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Align s1 and s2 with an inner join\n",
    "a, b = s1.align(s2, join='inner')\n",
    "side_by_side(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1          A    NaN      \n",
      "B    2          B    NaN      \n",
      "C    3          C    3.0      \n",
      "D    4          D    4.0      \n",
      "E    5          E    5.0      \n",
      "F    6          F    6.0      \n",
      "G    7          G    NaN      \n",
      "H    8          H    NaN      \n",
      "I    9          I    NaN      \n",
      "dtype: int64    dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Align s1 and s2 with a left join\n",
    "a, b = s1.align(s2, join='left')\n",
    "side_by_side(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D    4.0          D     4     \n",
      "E    5.0          E     5     \n",
      "F    6.0          F     6     \n",
      "P    0.0          P    14     \n",
      "Q    0.0          Q    15     \n",
      "R    0.0          R    16     \n",
      "C    3.0          C     3     \n",
      "Z    0.0          Z    18     \n",
      "Y    0.0          Y    19     \n",
      "dtype: float64    dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Align s1 and s2 with a right join and set value to 0 if NaN\n",
    "a, b = s1.align(s2, join='right', fill_value=0)\n",
    "side_by_side(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df=pd.DataFrame(np.random.randint(1,9,(3,3)),['A','B','C'],['X','Y','Z']) # pd.DataFrame(data,row_index,column_index)\n",
    "print(\"Dataframe:\\n\",df)\n",
    "\n",
    "# Functions in dataframes are quite similar to series\n",
    "\n",
    "print('\\nIndexes of DataFrame:\\n',df.index)\n",
    "print('\\nValues of DataFrame:\\n',df.values)\n",
    "print('\\nClass of DataFrame: ',type(df))\n",
    "\n",
    "# Extracting elements\n",
    "print('\\nAccessing a specific column (Y)\\n',df['Y']) # retrieving a column\n",
    "print('\\nAccessing multiple columns (X and Z)\\n',df[['X','Z']]) # retrieving multiple columns by passing a list of names of columns\n",
    "print('\\nAccessing a specific row (B)\\n',df.loc['B']) # df.loc(row_name) extracts the row elements\n",
    "print('\\nAccessing a specific row using index (C)\\n',df.iloc[2]) # df.iloc(index) extracts the row elements index-wise\n",
    "print('\\nAccessing a specific element (B,Z) using loc: ',df.loc['B','Z']) # df.loc(row_index,column_index) extracts the specific element\n",
    "print('\\nAccessing a specific element (C,Z) using iloc: ',df.iloc[2,2]) # df.iloc[Row,Column]\n",
    "\n",
    "print(\"\\nFiltering using between():\\n\",df[df['Z'].between(4,9)])\n",
    "print(\"\\nChecking null values:\\n\",df.isnull())\n",
    "print(\"\\nChecking non-null values:\\n\",df.notnull())\n",
    "\n",
    "m=df.as_matrix()\n",
    "print(\"\\nConverting Dataframe into Matrix: \\n\",m)\n",
    "print('\\nClass of Matrix: ',type(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/jamwine/Data/master/Class%20Result.csv\"\n",
    "df=pd.read_csv(url) # pd.read_csv(file) is used to read csv file and return it as a dataframe\n",
    "print('Dataset:\\n',df.head(3)) # the first line of the data is considered as column headings\n",
    "df=pd.read_csv(url, usecols=['Exam1','Exam2','Exam3']) # usecols describes which columns are to be added in the dataframe \n",
    "print('\\nDataset with specified columns using usecols:\\n',df.head(3))\n",
    "\n",
    "print(\"\\nDimension of dataframe: \",df.ndim)\n",
    "print(\"Shape of dataframe: \",df.shape) \n",
    "print(\"Size of dataframe: \",df.size) # No. of elements\n",
    "print('\\nColumns of DataFrame:\\n',df.columns)\n",
    "print('\\nDataFrame Axes:\\n',df.axes) # gives information about both indexes and values\n",
    "print(\"\\nSum of elements with columns as index:\\n\",df.sum()) # default axis=0\n",
    "print(\"\\nSum of elements with rows as index:\\n\",df.sum(axis=1).head(3))\n",
    "\n",
    "# Defining new column\n",
    "df['Total_Marks']=df['Exam1']+df['Exam2']+df['Exam3']\n",
    "print('\\nNew Column Total_Marks is added:\\n', df.head(3))\n",
    "\n",
    "df.insert(4,'Class',\"Class 7th\") # df.insert(loc, column, value) inserts a new column at the speciifed index location in dataframe\n",
    "print(\"\\nNew Column Class is introduced using insert():\\n\",df.head(3))\n",
    "\n",
    "df.drop('Class',axis=1) # df.drop(name,axis) is used to remove the specified rows/columns \n",
    "print(\"\\nAlthough the column Class is dropped, it still remains intact inside the original dataframe.\\n\",df.head(3))\n",
    "\n",
    "df.drop('Class',axis=1, inplace=True) # inplace attribute is used to overwrite the new data into the original dataframe \n",
    "print(\"\\nAfter using the inplace attribute, it removes the column Class permanently from the original dataframe.\\n\",df.head(3))\n",
    "\n",
    "del df['Total_Marks'] # del is permanent in nature unlike inplace attribute\n",
    "print(\"\\nColumn Total_Marks is deleted using del()\\n\",df.head(3))\n",
    "\n",
    "print(\"\\nFiltering dataset using conditions:\\n\",df[(df['Exam1']<220) & ((df['Exam2']>220) | (df['Exam3']>220))],'\\n') # returning dataframe, filtering the dataset \n",
    "print(\"Filering using isin():\\n\",df[df['Exam1'].isin([229,225,221,223,227])],'\\n') # df['column_name'].isin([elements]) checks for the list of elements in the dataset and returns the boolean value \n",
    "print(\"Filtering using between():\\n\",df[df['Exam3'].between(205,210)],\"\\n\")\n",
    "\n",
    "df.info() # information about dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/jamwine/Data/master/Exam%20Result.csv\" \n",
    "# this dataset doesn't contains any column headings\n",
    "df1=pd.read_csv(url,header=None) # header=None is used to prevent data in first row to be determined as the column headings\n",
    "print('Dataset without column headings:\\n',df1.head(3),'\\n')\n",
    "\n",
    "df1.dropna() # dropna() drops all rows containing any one NaN value, data is removed temporarily until we mention the parameter inplace=True\n",
    "df1.dropna(axis=1, how='all',inplace=True) # dropping columns which contains all its values as NaN (how='all') \n",
    "print('Dataset after removing all columns containing NaN values:\\n',df1.head(3))\n",
    "\n",
    "headers=['A','B','C','D','E','F','G','H','I','J','K','L','M']\n",
    "df1.columns=headers # assign the columns with a header\n",
    "print('\\nDataset after assigning columns:\\n',df1.head(3))\n",
    "\n",
    "# sep parameter describes the separator to be (,)  and index=False doesn't includes index to be stored in dataset\n",
    "df1.to_csv(\"Test.csv\", sep=',', index=False) # saving dataframe to test.csv file\n",
    "\n",
    "# renaming columns of dataframe\n",
    "print(\"\\nColumns of dataframe:\",df1.columns)\n",
    "df1.rename(columns={\"K\":\"P\",\"L\":\"Q\",\"M\":\"R\"},inplace=True) # rename(index, columns) is used to rename \n",
    "print(\"Columns of dataframe after renaming:\",df1.columns)\n",
    "\n",
    "df1.drop(labels=[\"R\"],axis=1,inplace=True) # drop() is used to remove elements\n",
    "print(\"\\nDataframe after dropping column R using drop():\\n\",df1.head(3))\n",
    "\n",
    "a=df1.pop(\"Q\")\n",
    "print(\"\\nDataframe after dropping column Q using pop():\\n\",df1.head(3))\n",
    "print(\"\\nValues of column Q in a list:\", list(a))\n",
    "\n",
    "del df1[\"P\"]\n",
    "print(\"\\nDataframe after dropping column P using del:\\n\",df1.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# the above cell has created a file test.csv\n",
    "a=pd.read_csv(\"Test.csv\",index_col='L') # index_col defines which column is to be used as index\n",
    "a.sort_index(inplace=True) # data is sorted upon index values\n",
    "print('Data extracted with index as column L using index_col\\n',a.head(3),'\\n')\n",
    "\n",
    "a.reset_index(inplace=True) # resets index for the dataframe\n",
    "a.set_index(\"M\",inplace=True) # sets index for the dataframe\n",
    "print('Data extracted with index as column M using set_index()\\n',a.head(3),'\\n')\n",
    "\n",
    "# if a single column is required from the data\n",
    "a=pd.read_csv(\"Test.csv\",usecols=['M']) # returns dataframe\n",
    "print('Single column extracted as a dataframe\\n',a.head(3),'\\n')\n",
    "\n",
    "a=pd.read_csv(\"Test.csv\",usecols=['M'],squeeze=True) # squeeze=True returns a series only if one column is extracted from the dataset\n",
    "print('Single column extracted as a series using squeeze\\n',a.head(3),'\\n')\n",
    "\n",
    "# MONTHLY EXPENDITURE Dataset\n",
    "url=\"https://raw.githubusercontent.com/jamwine/Data/master/Monthly%20Expenditure.csv\"\n",
    "b=pd.read_csv(url)\n",
    "print(\"Monthly Expenditure:\\n\",b.head(3))\n",
    "\n",
    "c=b.describe(include=\"all\") # include=\"all\" describes the properties for string objects additionally \n",
    "print('\\nDescription of the above dataset:\\n',c)\n",
    "\n",
    "c.dropna(subset=[\"Day\"],inplace=True) # checks for null values only in those columns which are specified in subset\n",
    "print('\\nDropping rows containing NaN values in Day column using subset:\\n',c)\n",
    "\n",
    "c.fillna(0,inplace=True) # replaces NaN in the column with 0\n",
    "print('\\nChanging NaN values to 0:\\n',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/jamwine/Data/master/Monthly%20Expenditure.csv\"\n",
    "a=pd.read_csv(url)\n",
    "print(\"Dataset:\\n\",a.head(3))\n",
    "\n",
    "print('\\nDatatype of columns (Raw data):\\n',a.dtypes) # dtypes attribute returns the datatype of all columns\n",
    "a['Date']=pd.to_datetime(a['Date'])  # converts string (object) column to date\n",
    "print('\\nDatatype of column Date  (After changing):\\n',a.dtypes)\n",
    "\n",
    "# (alternative method for converting strings into dates)\n",
    "b=pd.read_csv(url, parse_dates=[\"Date\"]) # parse_date parameter converts columns into dates\n",
    "print(\"\\nSample Dataset:\\n\",b.head(3))\n",
    "\n",
    "b[\"Category\"]=b[\"Category\"].astype('category') # astype('category') is used to convert string objects into category\n",
    "b[\"Cost\"]=b[\"Cost\"].astype(\"float\") # astype('float') is used to convert int into float\n",
    "print('\\nDatatype of columns is changed using parse_dates parameter and astype()\\n',b.dtypes)\n",
    "\n",
    "print('\\nUnique values in \\'Category\\' column are:\\n',b['Category'].unique()) # returns an array of unique values\n",
    "print('\\nLength of Unique values in \\'Category\\' column (skipping NaN)',b['Category'].nunique()) # returns length of unique values (doesn't counts NaN Values)\n",
    "print('Length of Unique values in \\'Day\\' column (counting NaN):',b['Day'].nunique(dropna=False)) # returns length of unique values (counts NaN Values as dropna=False)\n",
    "\n",
    "print('\\nDataframe after sorting multiple columns:\\n',b.sort_values([\"Cost\",\"Category\"],ascending=[False,True]).head(3)) # sorting multiple columns using sort_values\n",
    "\n",
    "b[\"Rank\"]=b[\"Cost\"].rank(ascending=False) # rank() is used to assign ranks, ascending=False means the greatest will be ranked at first\n",
    "print('\\nDataframe after creating a rank column:\\n',b.head(3))\n",
    "\n",
    "# filtering dataframe based on certain condition\n",
    "# Method 1\n",
    "mask1=b['Cost']>1000\n",
    "mask2=b['Category']==\"Rent\"\n",
    "print(\"\\nDataframe after applying filter:\\n\",b[mask1 & mask2])\n",
    "\n",
    "# Method 2\n",
    "print(\"\\nDataframe after applying filter using query():\\n\",b.query(\"Cost>1000 & Category=='Rent'\"))\n",
    "\n",
    "# Method 3\n",
    "print(\"\\nDataframe after applying filter using contains():\\n\",b[b[\"Category\"].str.contains(\"Rent\")].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/jamwine/Data/master/Class%20Result.csv\"\n",
    "a=pd.read_csv(url) # pd.read_csv(file) is used to read csv file and return it as a dataframe\n",
    "print('Dataset:\\n',a.head(3))\n",
    "\n",
    "x=(a[\"Total Marks\"].duplicated()) # returns true for duplicate values (first duplicate values are considered unique)\n",
    "print(\"\\nDuplicate Records (First duplicate records are skipped as they are considered unique):\\n\",a[x])\n",
    "\n",
    "y=(a[\"Total Marks\"].duplicated(keep=\"last\")) # returns true for duplicate values (last duplicate values are considered unique)\n",
    "print(\"\\nDuplicate Records (Last duplicate records are skipped as they are considered unique):\\n\",a[y])\n",
    "\n",
    "z=a[\"Total Marks\"].duplicated(keep=False) # returns all duplicate values including the first and the last value\n",
    "print(\"\\nAll Duplicate Records having same total marks:\\n\",a[z])\n",
    "\n",
    "p=~a[\"Exam1\"].duplicated(keep=False) # returns all unique values using tilde (~)\n",
    "print(\"\\nReturning a list of unique Exam1 marks: \",list(a[p][\"Exam1\"]))\n",
    "\n",
    "a.drop_duplicates(subset=[\"Exam1\"],keep=\"first\",inplace=True) # removes duplicates rows by checking values for columns mentioned in the subset\n",
    "print(\"\\nAfter removing duplicate records:\\n\",a.tail())\n",
    "\n",
    "print(\"\\nExtracting some rows from Dataframe:\\n\",a.loc[0:2],'\\n\\n',a.loc[[39,40,41]]) # loc[[list of rows]] is used to extract rows from the dataframe\n",
    "\n",
    "print(\"\\nExtracting a single row:\\n\",a.loc[3]) # returns series\n",
    "\n",
    "print(\"\\nSample rows from dataframe:\\n\",a.sample(frac=0.1)) # frac=0.1 means 10% of the dataset will be returned as sample\n",
    "print(\"\\nSample columns from dataframe:\\n\",a.sample(2,axis=1).tail(3))\n",
    "\n",
    "print(\"\\nExtracting two largest rows from dataframe:\\n\",a.nlargest(2,\"Total Marks\")) # nlargest(number,column to be sorted)\n",
    "print(\"\\nExtracting two smallest rows from dataframe:\\n\",a.nsmallest(2,\"Total Marks\")) # nsmallest(number,column to be sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/jamwine/Data/master/Class%20Result.csv\"\n",
    "a=pd.read_csv(url) # pd.read_csv(file) is used to read csv file and return it as a dataframe\n",
    "\n",
    "# defining function\n",
    "def grade(row):\n",
    "    if row[-1]>=90:\n",
    "        s=\"Grade A\"\n",
    "    elif row[-1]>=80:\n",
    "        s=\"Grade B\"\n",
    "    elif row[-1]>=70:\n",
    "        s=\"Grade C\"\n",
    "    else:\n",
    "        s=\"Grade D\"\n",
    "    return s\n",
    "\n",
    "a[\"Grade\"]=a.apply(grade,axis=\"columns\") # apply() is used to apply function on all elements in the dataset\n",
    "print(a.head())\n",
    "\n",
    "# changing an element in column\n",
    "b=a[\"Exam2\"] # series is extracted\n",
    "print(\"\\nSeries before changing an element in column Exam2:\\n\",b.head(3)) # index 1 has value 163\n",
    "b[1]=170 # element is modified\n",
    "print(\"\\nSeries after changing an element in column Exam2:\\n\",b.head(3)) # index 1 has a new value 170\n",
    "print(\"\\nDataframe is also affected by the above change:\\n\",a.head(3)) #  index 1 is changed to new value 170 from 163\n",
    "\n",
    "# in order to prevent values to be changed from original dataframe, copy() is used\n",
    "c=a[\"Exam1\"].copy()\n",
    "print(\"\\nSeries before change in column Exam1:\\n\",c.head(3)) # index 1 has value 223\n",
    "c[1]=230 # element is modified\n",
    "print(\"\\nSeries after change in column Exam1:\\n\",c.head(3)) # index 1 has a new value 230\n",
    "print(\"\\nDataframe is not affected by the above change:\\n\",a.head(3)) #  index 1 is not changed, still retains old value 221\n",
    "\n",
    "print(\"\\nCount of Grades:\\n\",a[\"Grade\"].str.split(\" \").str.get(1).value_counts()) # str.split() is used to split on series, str.get() extracts the element from the returned array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bins (converting quantitative values into categorical values)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/jamwine/Data/master/Class%20Result.csv\"\n",
    "a=pd.read_csv(url)\n",
    "print(\"Dataset:\\n\",a.head(3))\n",
    "bins=np.linspace(min(a[\"Total Marks\"]),max(a[\"Total Marks\"]),4)\n",
    "print(\"\\nBins:\",list(bins))\n",
    "\n",
    "a[\"Grade\"]=pd.cut(a[\"Total Marks\"],bins) # pd.cut(column,bins) is used to create bins on the specified column\n",
    "# by default, parameter right=true, max value is included while min value is not included\n",
    "print(\"\\nMaximum value is included:\\n\",a[8:11]) \n",
    "print(\"\\nMinimum value is not included:\\n\",a[30:33])\n",
    "\n",
    "# right=false is used to exclude the right-most values, max value is excluded while min value is included\n",
    "a[\"Grade\"]=pd.cut(a[\"Total Marks\"],bins,right=False)  \n",
    "print(\"\\nMaximum value is not included:\\n\",a[8:11])\n",
    "print(\"\\nMinimum value is included:\\n\",a[30:33])\n",
    "\n",
    "# include_lowest parameter is used to include the lowest value in the bin, both min and max value is included\n",
    "a[\"Grade\"]=pd.cut(a[\"Total Marks\"],bins,include_lowest =True)\n",
    "print(\"\\nMaximum value is included:\\n\",a[8:11])\n",
    "print(\"\\nMinimum value is included:\\n\",a[30:33])\n",
    "\n",
    "bin_names=[\"Grade C\",\"Grade B\",\"Grade A\"]\n",
    "a[\"Grade\"]=pd.cut(a[\"Total Marks\"],bins,include_lowest =True,labels=bin_names)\n",
    "print(\"\\nGrade with labels:\\n\",a.head(3))\n",
    "\n",
    "# converting categorical values into quantitative values\n",
    "url=\"https://raw.githubusercontent.com/jamwine/Data/master/Monthly%20Expenditure.csv\"\n",
    "b=pd.read_csv(url)\n",
    "print(\"\\nDataset:\\n\",b.head(3))\n",
    "print(\"\\nConverting categorical value into quantitative value: \\n\",pd.get_dummies(b[\"Day\"].sample(5))) # get_dummies(column) is used to convert each unique categorical value in the column into quantitative value\n",
    "\n",
    "pd.options.display.max_columns=8 # this defines the number of columns displayed while printing a dataframe\n",
    "a.transpose() # transpose of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names=[\"Trishant Dev\",\"Bhaskar Kaushik\",\"Sakshi Singh\",\"Dhruv Garg\",\"Yogesh Goel\",\"Pankaj Agarwaal\",\"Shaurya Khurana\", \"Akshay Kumar Kusneniwar\"]\n",
    "a=pd.DataFrame(names,columns=[\"Name\"])\n",
    "print(a[\"Name\"].str.split(\" \", expand=True).tail(3)) # expand=True splits the strings and returns the result in different columns; None is for missing values\n",
    "\n",
    "a[\"Dep\"]=\"Electronics\" \n",
    "a[[\"First Name\",\"Last Name\"]]=a[\"Name\"].str.split(\" \", expand=True, n=1) # n=1 splits the string once, thus returning two columns only\n",
    "print(\"\\n\",a)\n",
    "\n",
    "# creating multi_index using zip()\n",
    "x=[\"Team_1\",\"Team_1\",\"Team_1\",\"Team_1\",\"Team_2\",\"Team_2\",\"Team_2\",\"Team_2\"]\n",
    "y=[1,2,3,4,1,2,3,4]\n",
    "z=list(zip(x,y)) \n",
    "multi_index=pd.MultiIndex.from_tuples(z)\n",
    "\n",
    "a.set_index(multi_index,inplace=True) # multi-index (team and project) for each row in dataframe\n",
    "print(\"\\nMulti-indexes:\\n\",a.index)\n",
    "\n",
    "print(\"\\nExtracting index level-wise:\\n\",\"Index first level:\\n\",a.index.get_level_values(0),\"\\nIndex second level:\\n\",a.index.get_level_values(1)) # get_level_values() is used to extract index level-wise\n",
    "a.index.set_names([\"Team\",\"Project\"],inplace=True) # index name changed from Dep to Department\n",
    "\n",
    "print(\"\\nSorting Indexes:\\n\",a.sort_index(ascending=False).tail(3))\n",
    "\n",
    "print(\"\\nExtracting columns from multi-index using loc():\\n\",a.loc[(\"Team_2\",3)]) # a.loc[(multi-index)) returns all columns of the specified index\n",
    "print(\"\\nExtracting columns from multi-index using iloc():\\n\",a.iloc[(7)])\n",
    "\n",
    "print(\"\\nExtracting single column value from multi-index:\",a.loc[(\"Team_1\",2),\"Last Name\"]) # returns value for specified column\n",
    "\n",
    "print(\"\\nExtracting elements with xs():\\n\",a.xs(2,level=\"Project\")) # xs() is used to extract from index\n",
    "\n",
    "print(\"\\nAfter Swapping Index:\\n\",a.swaplevel().head(3)) # in order to ensure efficiency, we should have common indexes at outer levels\n",
    "\n",
    "b=a.stack()\n",
    "print(\"\\nDataframe is converted into series:\\n\",b.head(4)) # stack() is used to return one column for whole dataframe\n",
    "print(\"\\nSeries is converted into dataframe using to_frame():\\n\",b.to_frame().head(4)) # to_frame() returns dataframe with indexes and columns\n",
    "print(\"\\nSeries is converted into dataframe using unstack():\\n\",b.unstack().head(3)) # unstack() is used to unstack the stacked column\n",
    "b.unstack(\"Project\") # unstack(column_index|column name) is used to define table-like structure by unstacking the specified column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/jamwine/Data/master/Test%20Data.csv\"\n",
    "a=pd.read_csv(url,parse_dates=[\"Date\"])\n",
    "print(\"Dataset having length:\",len(a),\"\\n\",a.head(3))\n",
    "\n",
    "b=a.pivot(index=\"Date\",columns=\"Country\",values=\"Cost\") # pivot() is used to reshape the dataset, index must be unique\n",
    "print(\"\\nDataset using pivot() having new length:\",len(b),\"\\n\",b.head())\n",
    "print(\"\\nDataset is unpivoted using melt():\\n\",pd.melt(b,value_name=\"Cost\").head(3)) # melt() is used to unpivot the dataset in separate rows\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/jamwine/Data/master/Monthly%20Expenditure.csv\"\n",
    "c=pd.read_csv(url)\n",
    "print(\"\\nDataset:\\n\",c.head(3))\n",
    "# pivot_table is used to aggregate data based on our specified conditions, values represents data, index can be single/multi-level,columns represents unique categories and can be single/multi-level\n",
    "print(\"\\nData extracted using pivot_table():\\n\",np.around(c.pivot_table(values=\"Cost\",index=[\"Day\",\"Date\"],columns=\"Category\",aggfunc=\"sum\",fill_value=0),0))\n",
    "print(\"\\nData extracted using pivot_table():\\n\",np.around(c.pivot_table(values=\"Cost\",index=\"Category\",aggfunc=\"sum\",fill_value=0),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/jamwine/Data/master/Monthly%20Expenditure.csv\"\n",
    "a=pd.read_csv(url,parse_dates=[\"Date\"])\n",
    "print(\"Dataset:\",type(a),\"\\n\",a.head(3))\n",
    "\n",
    "b=a.groupby(\"Category\") # groupby() is used for grouping dataset by the specified column\n",
    "# length returns the unique columns in groupby() and size returns the number of rows in each category\n",
    "print(\"\\nDataset with groupby():\",type(b),\"having length:\",len(b), \"and size:\\n\",b.size())\n",
    "\n",
    "print(\"\\nFirst rows in each category:\\n\",b.first()) # first() returns the first occurences of rows in each category\n",
    "print(\"\\nLast rows in each category:\\n\",b.last()) # last() returns the last occurences of rows in each category\n",
    "\n",
    "print(\"\\nGrouping of dataset:\\n\",b.groups) # groups parameter returns the dictionary with each category as key and index of each row as value\n",
    "\n",
    "print(\"\\nRetrieving a category 'Basics' from group:\\n\",b.get_group(\"Basics\")) # get_group(category_value) returns all the entries of specified value\n",
    "\n",
    "print(\"\\nMax. in each category:\\n\",b.max()) # returns maximum in each category\n",
    "print(\"\\nMin. in each category:\\n\",b.min()) # returns minimum in each category\n",
    "\n",
    "print(\"\\nSum of each category:\\n\",b.sum())\n",
    "print(\"\\nAggregation of various operations in each category:\\n\",b.agg([\"sum\",\"mean\",\"max\",\"min\"])) # agg() is used to perform operations on the grouped dataset\n",
    "\n",
    "c=pd.DataFrame(columns=a.columns)\n",
    "for category,data in b:\n",
    "    c=c.append(data.nlargest(1,\"Cost\")) # dataframe is created keeping maximum value entries in each category\n",
    "print(\"\\nNew DataFrame with top values:\\n\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url1=\"https://raw.githubusercontent.com/jamwine/Data/master/Monthly%20Expenditure.csv\"\n",
    "url2=\"https://raw.githubusercontent.com/jamwine/Data/master/Monthly%20Expenditure%202.csv\"\n",
    "a=pd.read_csv(url1)\n",
    "b=pd.read_csv(url2)\n",
    "c=pd.concat([a,b]) # concat([data1,data2]) joins two data sources\n",
    "print(\"Length of a:\",len(a),\"\\nLength of b:\",len(b),\"\\nLength of c:\",len(c))\n",
    "\n",
    "print(\"\\nSample of dataset:\\n\",c.tail(3))\n",
    "\n",
    "c=pd.concat([a,b],ignore_index=True) # ignore_index parameter is used for indexing data in an efficient manner by concatenating two datasets\n",
    "print(\"\\nSample of dataset with ignore_index parameter:\\n\",c.tail(3))\n",
    "\n",
    "c=pd.concat([a,b], keys=[\"A\",\"B\"]) # index of each dataset is assigned using the keys\n",
    "print(\"\\nSample of dataset with keys defined for each dataset:\\n\",c.head(3),\"\\n\",c.tail(3))\n",
    "\n",
    "print(\"\\nExtracting single data record:\\n\",c.ix[\"A\",21],\"\\n\\n\",c.iloc[182])\n",
    "\n",
    "d=b.append(a,ignore_index=True) # another method to join two datasets\n",
    "print(\"\\nDatasets are appended using append():\\n\",d.head(3))\n",
    "\n",
    "inner=a.merge(b,how=\"inner\",on=[\"Day\",\"Category\",\"Expenditure\",\"Cost\"]) # inner join using merge(),'how' parameter defines type of join,'on' parameter takes multiple columns for joining\n",
    "print(\"\\nInner join using merge():\\n\",inner.head(3))\n",
    "\n",
    "inner=a.merge(b,how=\"inner\",on=[\"Day\",\"Category\",\"Expenditure\",\"Cost\"],suffixes=[\"_A\",\"_B\"]) # 'suffixes' parameter is used to define columns from two different datasets\n",
    "print(\"\\nInner join with suffixes parameter:\\n\",inner.head(3))\n",
    "\n",
    "outer=a.merge(b,how=\"outer\",on=[\"Day\",\"Expenditure\",\"Category\"],suffixes=[\"_A\",\"_B\"])\n",
    "print(\"\\nOuter join using merge():\\n\",outer.head(3))\n",
    "\n",
    "outer_with_indicator=a.merge(b,how=\"outer\",on=[\"Day\",\"Expenditure\",\"Category\"],suffixes=[\"_A\",\"_B\"],indicator=True) # indicator parameter denotes the data from a particular dataset\n",
    "print(\"\\nOuter join with indicator:\\n\",outer_with_indicator.head(3)) # _merge column represents data from a particular dataset\n",
    "print(\"\\nSummary of outer join:\\n\",outer_with_indicator[\"_merge\"].value_counts()) # represents summary of records taken from each dataset in the outer join\n",
    "\n",
    "# similarly, merge() is used for left  & right join.'sort' parameter is used to sort the resulting dataset based on the matched column\n",
    "# if the two datasets have a different column name on which the data is to be joined, we use 'left_on' and 'right_on' parameters\n",
    "# 'left_index' and 'right_index' parameters are used to join datasets when the dataset contains the column as index \n",
    "# in order to attach a new column which is present in different dataset, we use join() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "a=dt.datetime(1995,6,26,8,5,25)\n",
    "print(\"Date and Time using datetime():\",a, type(a))\n",
    "b=\"05/12/1994 8:15:30 AM\"\n",
    "print(\"Date and Time as a string:\",b, type(b))\n",
    "print(\"Date and Time using timestamp():\",pd.Timestamp(a), pd.Timestamp(b), type(pd.Timestamp(b))) # timestamp objects\n",
    "\n",
    "dates=[dt.date(2017,6,26),\"2015/11/30\",\"2013-1-6\",\"Aug 15th, 2018\",\"3rd July 1947\",\"2015\"]\n",
    "print(\"\\nDates:\",dates, type(dates))\n",
    "print(\"\\nDate and Time using to_datetime():\",pd.to_datetime(dates)) # to_datetime() is used to convert strings of various formats into datetime\n",
    "\n",
    "x=pd.DatetimeIndex(dates) # datetimeindex object contains list of timestamp objects\n",
    "print(\"\\nDate and Time using DatetimeIndex():\",x, type(x)) \n",
    "print(\"\\nExtracting element from the list:\",x[1])\n",
    "\n",
    "dates=[dt.date(2017,6,26),\"2015/11/30\",\"2013-1-6\",\"Aug 15th, 2018\",\"3rd July 1947\",\"2015\",\"31st April 2014\",\"Sample_text\",\"1345673680\"]\n",
    "y=pd.Series(dates)\n",
    "print(\"\\nSeries:\\n\",y)\n",
    "\n",
    "z=pd.to_datetime(y,errors='coerce') # errors='coerce' is used to return NaT for invalid dates without throwing any error\n",
    "print(\"\\nSeries converted into dates:\\n\",z)\n",
    "\n",
    "print(\"\\nWeekday Name:\\n\",z.dt.weekday_name) # dt.weekday_name parameter returns day of that date\n",
    "\n",
    "print(\"\\nMonth End:\\n\",z.dt.is_month_end) # dt.is_quarter_end parameter returns boolean value of that date\n",
    "\n",
    "print(\"\\nTime in seconds since 1970-01-01 is converted into timestamp:\",pd.to_datetime(\"1465673680\",unit='s')) # unit='s' represents seconds\n",
    "\n",
    "print(\"\\nDates using date_range():\\n\",pd.date_range(start=\"26th Oct 2018\",end=\"6th Nov 2018\")) # date_range() is used to return a range of dates between two intervals\n",
    "print(pd.date_range(start=\"26th Oct 2018\",end=\"6th Nov 2018\",freq='2D')) # frequency means 2 days\n",
    "print(pd.date_range(start=\"26th Oct 2018\",end=\"6th Nov 2018\",freq='B')) # frequency means business days\n",
    "print(pd.date_range(start=\"26th Oct 2018\",end=\"13th Nov 2018\",freq='W')) # frequency means week (displaying sunday by default)\n",
    "print(pd.date_range(start=\"26th Oct 2018\",end=\"13th Nov 2018\",freq='M')) # frequency means Month's end\n",
    "print(pd.date_range(start=\"26th Oct 2018\",periods=8,freq='D')) # periods parameter denotes number of dates\n",
    "print(pd.date_range(start=\"26th Oct 2018\",periods=4,freq='5H')) # frequency means every 5 hours, will start from the start date\n",
    "print(pd.date_range(end=\"26th Oct 2018\",periods=4,freq='2D')) # frequency means every 2 days, will end at the end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user nsepy # library is downloaded which is not available by default\n",
    "import nsepy # nsepy modules helps in importing datasets from Indian Stock Market Exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nsepy import get_history\n",
    "import time\n",
    "\n",
    "today=time.strftime(\"%x\") # returns current date\n",
    "sbi=get_history(symbol=\"SBIN\", start=pd.to_datetime('2014-01-01').date(), end=pd.to_datetime(today).date())\n",
    "sbi.index=pd.to_datetime(sbi.index)\n",
    "sbi_share=sbi[[\"Prev Close\",\"Open\",\"High\",\"Low\",\"Close\",\"Trades\"]]\n",
    "print(sbi_share.truncate(before=\"2018-09-01\",after=\"2018-09-12\")) # truncate() is used to fetch data for the specified range from the dataset \n",
    "\n",
    "birthdays=pd.date_range(start=\"2014-05-12\", end=today,freq=pd.DateOffset(years=1))\n",
    "print(\"\\nMy birthdays from 2014 onwards:\")\n",
    "for day in birthdays:\n",
    "    print(day.date(),day.weekday_name) # printing birthdates along with days\n",
    "\n",
    "# stock prices on birthday\n",
    "sbi_birthday=sbi[sbi.index.isin(birthdays)][[\"Prev Close\",\"Open\",\"High\",\"Low\",\"Close\",\"Trades\"]]\n",
    "print(\"\\nSBI Share Price on birthdays: \\n\",sbi_birthday)\n",
    "\n",
    "print(\"\\nOriginal Index:\\n\",sbi.index)\n",
    "print(\"\\nNew Index:\\n\",sbi.index+ pd.DateOffset(days=5,months=2,years=1)) # here, pd.DateOffset() is used to add the specified fields in the original index\n",
    "\n",
    "# pd.tseries.offsets is imported separatedly for avoiding large syntax\n",
    "from pandas.tseries.offsets import *\n",
    "print(\"\\nNew Index using offsets:\\n\",sbi.index+QuarterEnd())\n",
    "# other functions are MonthEnd(), BMonthEnd(), OuarterBegin(), YearEnd(), YearBegin() etc.\n",
    "\n",
    "time_1=pd.Timestamp(\"2018-09-13 21:09:30\")\n",
    "time_2=pd.Timestamp(\"2018-08-16 16:26:20\")\n",
    "print(\"\\nTime_1:\",time_1)\n",
    "print(\"Time_2:\",time_2)\n",
    "\n",
    "print(\"\\nNegative Difference of two times:\",time_2-time_1)\n",
    "print(\"Positive Difference of two times:\",time_1-time_2)\n",
    "print(\"Type:\",type(time_1-time_2))\n",
    "\n",
    "a=pd.Timedelta(days=3, weeks=1, hours=4, minutes=20) # Timedelta() can be used like DateOffset() for various operations on time\n",
    "b=pd.Timedelta(\"16 days 11 hours 30 minutes 10 seconds\") # Timedelta() can be used like DateOffset() for various operations on time\n",
    "print(\"\\nTimedelta a:\",a)\n",
    "print(\"Timedelta b:\",b)\n",
    "\n",
    "print(\"\\nNew Time_1 after using pd.TimeDelta():\",time_1+a)\n",
    "print(\"New Time_2 after using pd.TimeDelta():\",time_2+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ep = pd.Panel()\n",
    "print(\"Empty Panel:\",ep)\n",
    "\n",
    "a=np.random.randint(1,10,(2,4,3))\n",
    "p=pd.Panel(a,['A','B'],['P','Q','R','S'],['X','Y','Z']) # three dimensional\n",
    "print(\"\\nPanel:\",p)\n",
    "\n",
    "print(\"\\nDimensions:\",p.ndim)\n",
    "print(\"Items:\",p.items)\n",
    "print(\"Major Axis:\",p.major_axis)\n",
    "print(\"Minor Axis:\",p.minor_axis)\n",
    "print(\"Axes:\",p.axes)\n",
    "print(\"Shape:\",p.shape)\n",
    "print(\"Size:\",p.size)\n",
    "\n",
    "print(\"\\nValues of Panel:\\n\",p.values)\n",
    "\n",
    "print(\"\\nExtracting item A:\\n\",p[\"A\"])\n",
    "\n",
    "print(\"\\nExtracting row R from item A using p.loc['A','R']:\\n\",p.loc[\"A\",\"R\"])\n",
    "\n",
    "print(\"\\nExtracting data from item A row R column Y using p.loc['A','R','Y']:\",p.loc[\"A\",\"R\",\"Y\"])\n",
    "print(\"\\nExtracting row R from item A using p.iloc[0,2]:\\n\",p.iloc[0,2])\n",
    "print('\\nExtracting data from item A row R column Y using p.iloc[0,2,1]:',p.iloc[0,2,1])\n",
    "\n",
    "df=p.to_frame()\n",
    "print(\"\\nPanel to Dataframe:\\n\",df.T)\n",
    "\n",
    "print(\"\\nDataframe to Panel:\\n\",df.to_panel())\n",
    "\n",
    "print(\"\\nValue of Row Q:\\n\",p.major_xs(\"Q\")) # data extracted by removing one dimension\n",
    "\n",
    "print(\"\\nValue of column Y:\\n\",p.minor_xs(\"Y\")) # data extracted by removing one dimension\n",
    "print(\"\\nTranspose of Panel:\",p.transpose(2,0,1)) #transpose(axes), axes are called using index (0 - items, 1 - major_axis, 2 - minor_axis)\n",
    "print(\"\\nSwapping Axis:\",p.swapaxes(\"items\",\"minor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading and writing .xlsx files\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names=[\"Jack\",\"John\",\"Jenny\",\"James\",\"Joseph\",\"Jim\",\"John\",\"Jasmine\"]\n",
    "city=[\"Dubai\",\"Delhi\",\"Paris\",\"Rome\",\"New York\",\"Mumbai\",\"San Jose\",\"London\"]\n",
    "age=list(np.random.randint(20,30,8))\n",
    "data={'Name':names,\n",
    "      'City':city,\n",
    "      'Age':age}\n",
    "students=pd.DataFrame(data)\n",
    "students=students[[\"Name\",\"City\",\"Age\"]] # ordering columns in data\n",
    "\n",
    "excel_file=pd.ExcelWriter(\"Students.xlsx\") # creating an excel file using ExcelWriter object\n",
    "students.to_excel(excel_file,sheet_name=\"Student Details\",index=False) # pushing dataframe to excel file\n",
    "excel_file.save() # saving excel file\n",
    "\n",
    "pd.read_excel(\"Students.xlsx\") # reading excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options and settings\n",
    "\n",
    "# pd.get_option('Option Name') fetches the value of option\n",
    "# pd.set_option('Option Name') sets the value of option\n",
    "\n",
    "print('pd.get_option(\"max_columns\"):',pd.get_option(\"max_columns\"))\n",
    "print('pd.get_option(\"max_rows\"):',pd.get_option(\"max_rows\"))\n",
    "\n",
    "pd.reset_option('max_rows') # resets the option value to default\n",
    "pd.set_option(\"max_rows\",10) # Setting new max_rows\n",
    "print('\\nPrinting Information about option:')\n",
    "pd.describe_option('max_rows')\n",
    "pd.set_option(\"precision\",2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
